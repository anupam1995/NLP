{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 - Implementing a tokenizer\n",
    "Implement a basic whitespace tokenizer in Python from scratch without the use of any NLP\n",
    "libraries. This tokenizer should drop whitespaces and create tokens for the following cases:\n",
    "\n",
    "(a) End-of-sentence (EOS) symbols, brackets and separators\n",
    "\n",
    "(b) Abbreviations - Assume those are only one of the following: Ph.D., Dr., M.Sc.\n",
    "\n",
    "(c) Special characters as in prices separated (i.e. $45.55)\n",
    "\n",
    "(d) Dates - Assume that they follow the format dd/mm/yy (i.e. 01/02/06)\n",
    "\n",
    "(e) URLs - Assume that they follow the format:\n",
    "http[s]://[...], (i.e. https://www.stanford.edu)\n",
    "\n",
    "(f) Hashtags separated (i.e. #nlproc)\n",
    "\n",
    "(g) Email addresses - Assume that they follow the format:\n",
    "name@domain.xyz (i.e. someOne@brown.edu)\n",
    "\n",
    "Apply your code on the test example below, which should yield the specified tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = \"He has a M.Sc. in Math and she has a Ph.D. in NLP. A session costs 45.55$ or $50.00. As of 01/02/06, please email X/Y at someone@brown.edu or visit http://www.stanford.edu and if link does not work try https://www.stanford.edu instead. #test#test2#nlproc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaTokenizer():\n",
    "    def __init__(self):\n",
    "        self.abbreviations = [\"Ph.D.\", \"Dr.\", \"M.Sc.\"]\n",
    "        self.abbr_pattern = r'Ph\\.D\\.|Dr\\.|M\\.Sc\\.'\n",
    "        self.separators = [\".\", \",\", \"/\", \"(\", \")\"]\n",
    "        self.separators_pattern = r'[.,/()!]'\n",
    "        self.date_pattern = r'\\b\\d{2}/\\d{2}/\\d{2,4}\\b'\n",
    "        self.prices_pattern = r'\\$\\d+(?:\\.\\d{1,2})?|\\d+(?:\\.\\d{1,2})?\\$'\n",
    "        self.urls_pattern = r'https?://(?:www\\.)?[a-zA-Z0-9\\-]+\\.[a-zA-Z]{2,}'\n",
    "        self.emails_pattern = r'[a-zA-Z0-9]+@[a-zA-Z0-9]+\\.[a-zA-Z]+'\n",
    "        self.hashtags_pattern = r'\\#[a-zA-Z0-9]+'\n",
    "        self.words = r'[a-zA-Z0-9]+'\n",
    "        self.combined_pattern = f'{self.date_pattern}|{self.abbr_pattern}|{self.prices_pattern}|{self.urls_pattern}|{self.emails_pattern}|{self.hashtags_pattern}|{self.words}|{self.separators_pattern}'\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        #get words from text\n",
    "        tokens = re.findall(self.combined_pattern, text)\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = VanillaTokenizer()\n",
    "tokens = tokenizer.tokenize(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['He',\n",
       " 'has',\n",
       " 'a',\n",
       " 'M.Sc.',\n",
       " 'in',\n",
       " 'Math',\n",
       " 'and',\n",
       " 'she',\n",
       " 'has',\n",
       " 'a',\n",
       " 'Ph.D.',\n",
       " 'in',\n",
       " 'NLP',\n",
       " '.',\n",
       " 'A',\n",
       " 'session',\n",
       " 'costs',\n",
       " '45.55$',\n",
       " 'or',\n",
       " '$50.00',\n",
       " '.',\n",
       " 'As',\n",
       " 'of',\n",
       " '01/02/06',\n",
       " ',',\n",
       " 'please',\n",
       " 'email',\n",
       " 'X',\n",
       " '/',\n",
       " 'Y',\n",
       " 'at',\n",
       " 'someone@brown.edu',\n",
       " 'or',\n",
       " 'visit',\n",
       " 'http://www.stanford.edu',\n",
       " 'and',\n",
       " 'if',\n",
       " 'link',\n",
       " 'does',\n",
       " 'not',\n",
       " 'work',\n",
       " 'try',\n",
       " 'https://www.stanford.edu',\n",
       " 'instead',\n",
       " '.',\n",
       " '#test',\n",
       " '#test2',\n",
       " '#nlproc']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "\n",
    "class BPETokenizer:\n",
    "    def __init__(self, num_merges=10):\n",
    "        self.num_merges = num_merges\n",
    "        self.bpe_codes = {}\n",
    "\n",
    "    def get_stats(self, corpus):\n",
    "        pairs = defaultdict(int)\n",
    "        for word, freq in corpus.items():\n",
    "            symbols = word.split()\n",
    "            for i in range(len(symbols) - 1):\n",
    "                pairs[(symbols[i], symbols[i + 1])] += freq\n",
    "        return pairs\n",
    "\n",
    "    def merge_vocab(self, pair, corpus):\n",
    "        new_vocab = {}\n",
    "        bigram = ' '.join(pair)\n",
    "        replacement = ''.join(pair)\n",
    "        for word in corpus:\n",
    "            new_word = word.replace(bigram, replacement)\n",
    "            new_vocab[new_word] = corpus[word]\n",
    "        return new_vocab\n",
    "\n",
    "    def train(self, corpus):\n",
    "        # Corpus format: {\"l o w </w>\": 5, ...}\n",
    "        for i in range(self.num_merges):\n",
    "            pairs = self.get_stats(corpus)\n",
    "            if not pairs:\n",
    "                break\n",
    "            best = max(pairs, key=pairs.get)\n",
    "            corpus = self.merge_vocab(best, corpus)\n",
    "            self.bpe_codes[best] = i\n",
    "        self.vocab = corpus\n",
    "\n",
    "    def tokenize(self, word):\n",
    "        word = list(word) + ['</w>']\n",
    "        while True:\n",
    "            pairs = [(word[i], word[i + 1]) for i in range(len(word) - 1)]\n",
    "            merge_candidates = [p for p in pairs if p in self.bpe_codes]\n",
    "            if not merge_candidates:\n",
    "                break\n",
    "            best = min(merge_candidates, key=lambda p: self.bpe_codes[p])\n",
    "            i = pairs.index(best)\n",
    "            word = word[:i] + [''.join(best)] + word[i+2:]\n",
    "        return word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['low', 'est</w>']\n"
     ]
    }
   ],
   "source": [
    "# Example training\n",
    "corpus = {\n",
    "    \"l o w </w>\": 5,\n",
    "    \"l o w e r </w>\": 2,\n",
    "    \"n e w e s t </w>\": 6,\n",
    "    \"w i d e s t </w>\": 3\n",
    "}\n",
    "\n",
    "tokenizer = BPETokenizer(num_merges=10)\n",
    "tokenizer.train(corpus)\n",
    "\n",
    "# Example tokenization\n",
    "print(tokenizer.tokenize(\"lowest\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'low</w>': 5, 'low e r </w>': 2, 'newest</w>': 6, 'wi d est</w>': 3}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('e', 's'): 0,\n",
       " ('es', 't'): 1,\n",
       " ('est', '</w>'): 2,\n",
       " ('l', 'o'): 3,\n",
       " ('lo', 'w'): 4,\n",
       " ('n', 'e'): 5,\n",
       " ('ne', 'w'): 6,\n",
       " ('new', 'est</w>'): 7,\n",
       " ('low', '</w>'): 8,\n",
       " ('w', 'i'): 9}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.bpe_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
