{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 - Implementing a tokenizer\n",
    "Implement a basic whitespace tokenizer in Python from scratch without the use of any NLP\n",
    "libraries. This tokenizer should drop whitespaces and create tokens for the following cases:\n",
    "\n",
    "(a) End-of-sentence (EOS) symbols, brackets and separators\n",
    "\n",
    "(b) Abbreviations - Assume those are only one of the following: Ph.D., Dr., M.Sc.\n",
    "\n",
    "(c) Special characters as in prices separated (i.e. $45.55)\n",
    "\n",
    "(d) Dates - Assume that they follow the format dd/mm/yy (i.e. 01/02/06)\n",
    "\n",
    "(e) URLs - Assume that they follow the format:\n",
    "http[s]://[...], (i.e. https://www.stanford.edu)\n",
    "\n",
    "(f) Hashtags separated (i.e. #nlproc)\n",
    "\n",
    "(g) Email addresses - Assume that they follow the format:\n",
    "name@domain.xyz (i.e. someOne@brown.edu)\n",
    "\n",
    "Apply your code on the test example below, which should yield the specified tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = \"He has a M.Sc. in Math and she has a Ph.D. in NLP. A session costs 45.55$ or $50.00. As of 01/02/06, please email X/Y at someone@brown.edu or visit http://www.stanford.edu and if link does not work try https://www.stanford.edu instead. #test#test2#nlproc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaTokenizer():\n",
    "    def __init__(self):\n",
    "        self.abbreviations = [\"Ph.D.\", \"Dr.\", \"M.Sc.\"]\n",
    "        self.abbr_pattern = r'Ph\\.D\\.|Dr\\.|M\\.Sc\\.'\n",
    "        self.separators = [\".\", \",\", \"/\", \"(\", \")\"]\n",
    "        self.separators_pattern = r'[.,/()!]'\n",
    "        self.date_pattern = r'\\b\\d{2}/\\d{2}/\\d{2,4}\\b'\n",
    "        self.prices_pattern = r'\\$\\d+(?:\\.\\d{1,2})?|\\d+(?:\\.\\d{1,2})?\\$'\n",
    "        self.urls_pattern = r'https?://(?:www\\.)?[a-zA-Z0-9\\-]+\\.[a-zA-Z]{2,}'\n",
    "        self.emails_pattern = r'[a-zA-Z0-9]+@[a-zA-Z0-9]+\\.[a-zA-Z]+'\n",
    "        self.hashtags_pattern = r'\\#[a-zA-Z0-9]+'\n",
    "        self.words = r'[a-zA-Z0-9]+'\n",
    "        self.combined_pattern = f'{self.date_pattern}|{self.abbr_pattern}|{self.prices_pattern}|{self.urls_pattern}|{self.emails_pattern}|{self.hashtags_pattern}|{self.words}|{self.separators_pattern}'\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        #get words from text\n",
    "        tokens = re.findall(self.combined_pattern, text)\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = VanillaTokenizer()\n",
    "tokens = tokenizer.tokenize(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['He',\n",
       " 'has',\n",
       " 'a',\n",
       " 'M.Sc.',\n",
       " 'in',\n",
       " 'Math',\n",
       " 'and',\n",
       " 'she',\n",
       " 'has',\n",
       " 'a',\n",
       " 'Ph.D.',\n",
       " 'in',\n",
       " 'NLP',\n",
       " '.',\n",
       " 'A',\n",
       " 'session',\n",
       " 'costs',\n",
       " '45.55$',\n",
       " 'or',\n",
       " '$50.00',\n",
       " '.',\n",
       " 'As',\n",
       " 'of',\n",
       " '01/02/06',\n",
       " ',',\n",
       " 'please',\n",
       " 'email',\n",
       " 'X',\n",
       " '/',\n",
       " 'Y',\n",
       " 'at',\n",
       " 'someone@brown.edu',\n",
       " 'or',\n",
       " 'visit',\n",
       " 'http://www.stanford.edu',\n",
       " 'and',\n",
       " 'if',\n",
       " 'link',\n",
       " 'does',\n",
       " 'not',\n",
       " 'work',\n",
       " 'try',\n",
       " 'https://www.stanford.edu',\n",
       " 'instead',\n",
       " '.',\n",
       " '#test',\n",
       " '#test2',\n",
       " '#nlproc']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2 - Implementing a BPE tokenizer\n",
    "Implement a Byte Pair Encoder (BPE) tokenizer as shown in the lecture and apply it to a\n",
    "sample text. You are free with your choice of libraries. You can assume that the corpus only\n",
    "consists of a list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class BPETokenizer:\n",
    "    def __init__(self, num_merges=10):\n",
    "        self.num_merges = num_merges\n",
    "        self.bpe_codes = {}\n",
    "\n",
    "    def get_stats(self, corpus):\n",
    "        pairs = defaultdict(int)\n",
    "        for word, freq in corpus.items():\n",
    "            symbols = word.split()\n",
    "            for i in range(len(symbols) - 1):\n",
    "                pairs[(symbols[i], symbols[i + 1])] += freq\n",
    "        return pairs\n",
    "\n",
    "    def merge_vocab(self, pair, corpus):\n",
    "        new_vocab = {}\n",
    "        bigram = ' '.join(pair)\n",
    "        replacement = ''.join(pair)\n",
    "        for word in corpus:\n",
    "            new_word = word.replace(bigram, replacement)\n",
    "            new_vocab[new_word] = corpus[word]\n",
    "        return new_vocab\n",
    "\n",
    "    def train(self, corpus):\n",
    "        # Corpus format: {\"l o w </w>\": 5, ...}\n",
    "        for i in range(self.num_merges):\n",
    "            pairs = self.get_stats(corpus)\n",
    "            if not pairs:\n",
    "                break\n",
    "            best = max(pairs, key=pairs.get)\n",
    "            corpus = self.merge_vocab(best, corpus)\n",
    "            self.bpe_codes[best] = i\n",
    "        self.vocab = corpus\n",
    "\n",
    "    def tokenize(self, word):\n",
    "        word = list(word) + ['</w>']\n",
    "        while True:\n",
    "            pairs = [(word[i], word[i + 1]) for i in range(len(word) - 1)]\n",
    "            merge_candidates = [p for p in pairs if p in self.bpe_codes]\n",
    "            if not merge_candidates:\n",
    "                break\n",
    "            best = min(merge_candidates, key=lambda p: self.bpe_codes[p])\n",
    "            i = pairs.index(best)\n",
    "            word = word[:i] + [''.join(best)] + word[i+2:]\n",
    "        return word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['low', 'est</w>']\n"
     ]
    }
   ],
   "source": [
    "# Example training\n",
    "corpus = {\n",
    "    \"l o w </w>\": 5,\n",
    "    \"l o w e r </w>\": 2,\n",
    "    \"n e w e s t </w>\": 6,\n",
    "    \"w i d e s t </w>\": 3\n",
    "}\n",
    "\n",
    "tokenizer = BPETokenizer(num_merges=10)\n",
    "tokenizer.train(corpus)\n",
    "\n",
    "# Example tokenization\n",
    "print(tokenizer.tokenize(\"lowest\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'low</w>': 5, 'low e r </w>': 2, 'newest</w>': 6, 'wi d est</w>': 3}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('e', 's'): 0,\n",
       " ('es', 't'): 1,\n",
       " ('est', '</w>'): 2,\n",
       " ('l', 'o'): 3,\n",
       " ('lo', 'w'): 4,\n",
       " ('n', 'e'): 5,\n",
       " ('ne', 'w'): 6,\n",
       " ('new', 'est</w>'): 7,\n",
       " ('low', '</w>'): 8,\n",
       " ('w', 'i'): 9}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.bpe_codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3 - Using pre-implemented tokenizers\n",
    "Use an existing tokenizer from the T5 Transformer or any other tokenizer of choice from the\n",
    "HuggingFace library. Apply the tokenizer to a text sample of choice. Compare the output of\n",
    "this tokenizer with the two tokenizers you implemented in the previous questions and explain\n",
    "the similarities and differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m tokens = tokenizer.tokenize(\u001b[43mtest_text\u001b[49m)\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(tokens)\n",
      "\u001b[31mNameError\u001b[39m: name 'test_text' is not defined"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(test_text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4 - RegEx\n",
    "Assume we have a lookup table named lookup storing abbreviation definitions. Using regular\n",
    "expressions in Python, write code that uses lookup to replace abbreviations in any given text\n",
    "with their full-text counterparts. Apply your code to the following snippet so that example\n",
    "is transformed to match target_output:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lookup = { ’doa’: ’dead on arrival’, ’gf’: ’girlfriend’, ’bf’: ’boyfriend’, ’btw’: ’by the way’, ’lol’: ’laughing out loud’, }\n",
    "example = \"I was lol when my bf’s phone was doa.\" \n",
    "target_output = \"I was laughing out loud when my boyfriend’s phone was dead on arrival.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re.compile('\\\\b(doa|gf|bf|btw|lol)\\\\b')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lookup = {\n",
    "    'doa': 'dead on arrival',\n",
    "    'gf': 'girlfriend',\n",
    "    'bf': 'boyfriend',\n",
    "    'btw': 'by the way',\n",
    "    'lol': 'laughing out loud',\n",
    "}\n",
    "\n",
    "example = \"I was lol when my bf’s phone was doa.\"\n",
    "\n",
    "# Pattern to match any abbreviation as whole word (case insensitive)\n",
    "pattern = re.compile(r'\\b(' + '|'.join(re.escape(key) for key in lookup.keys()) + r')\\b')\n",
    "\n",
    "print(pattern)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I was laughing out loud when my boyfriend’s phone was dead on arrival.\n"
     ]
    }
   ],
   "source": [
    "# Replace using lookup\n",
    "def replace(match):\n",
    "    word = match.group(0)\n",
    "    return lookup.get(word, word)\n",
    "\n",
    "# Apply substitution\n",
    "output = pattern.sub(replace, example)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/nlp/lib/python3.13/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/nlp/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁He', '▁has', '▁', 'a', '▁M', '.', 'S', 'c', '.', '▁in', '▁Math', '▁and', '▁', 'a', '▁Ph', '.', 'D', '.', '▁in', '▁N', 'LP', '.', '▁A', '▁session', '▁costs', '▁45', '.', '55', '$', '▁or', '▁$50', '.', '▁As', '▁of', '▁01', '/', '02', '/', '06', ',', '▁please', '▁email', '▁', 'X', '▁at', '▁someone', '@', 'brow', 'n', '.', 'e', 'du', '▁or', '▁visit', '▁http', '://', 'www', '.', 'stan', 'ford', '.', 'e', 'du', '▁(', 'if', '▁link', '▁does', '▁not', '▁work', '▁try', '▁https', '://', 'www', '.', 'stan', 'ford', '.', 'e', 'du', ').', '▁#', 'test', '#', 'test', '2', '#', 'n', 'l', 'pro', 'c', '</s>']\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "test_text = \"He has a M.Sc. in Math and a Ph.D. in NLP. A session costs 45.55$ or $50. As of 01/02/06, please email X at someone@brown.edu or visit http://www.stanford.edu (if link does not work try https://www.stanford.edu). #test#test2#nlproc\"\n",
    "print(tokenizer.convert_ids_to_tokens(tokenizer(test_text).input_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
